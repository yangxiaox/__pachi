## 蒙特卡洛树搜索
蒙特卡罗算法并不是一种算法的名称，而是对一类随机算法的特性的概括。它是一种通过随机取样进行判断并获得最优解的算法概括。但是由于围棋是进行随机取样的，所以通常并不能保证找到最优解，只能说是尽量找，根据“尽量”，把随机算法分成两类。

蒙特卡罗算法：采样越多，越近似最优解。
——尽量找好的，但不保证是最好的。

拉斯维加斯算法：采样越多，越有机会找到最优解。
——尽量找最好的，但不保证能找得到。

对于围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以选择蒙特卡洛式的算法。

蒙特卡罗树搜索(MCTS)会逐渐的建立一颗不对称的树。大概可以被分成四步并反复迭代。<font color=#0099ff>选择(Selection)，拓展(Expansion)，模拟(Simulation)，反向传播(Backpropagation)。

<font color=#>搜索树中的每一个节点包含了三个基本信息：代表的局面，被访问的次数，累计评分。

<font color=#0099ff>[1]选择(Selection)<font color=#>

><font color=#>在选择阶段，需要从根节点，也就是要做决策的局面R出发向下选择出一个最急迫需要被拓展的节点N，局面R是是每一次迭代中第一个被检查的节点；

>>对于被检查的局面而言，他可能有三种可能：          
* 1)该节点所有可行动作都已经被拓展过     
* 2)该节点有可行动作还未被拓展过     
* 3)这个节点游戏已经结束了(例如已经连成五子的五子棋局面)  
对于这三种可能：     
* 1)如果所有可行动作都已经被拓展过了，那么我们将使用UCB公式计算该节点所有子节点的UCB值，并找到值最大的一个子节点继续检查。反复向下迭代。     
* 2)如果被检查的局面依然存在没有被拓展的子节点(例如说某节点有20个可行动作，但是在搜索树中才创建了19个子节点)，那么我们认为这个节点就是本次迭代的的目标节点N，并找出N还未被拓展的动作A。执行步骤[2]     
* 3)如果被检查到的节点是一个游戏已经结束的节点。那么从该节点直接执行步骤[4]。每一个被检查的节点的被访问次数在这个阶段都会自增。在反复的迭代之后，我们将在搜索树的底端找到一个节点，来继续后面的步骤。
     
<font color=#0099ff>[2]拓展(Expansion)<font color=#>
><font color=#>在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点N，以及他一个尚未拓展的动作A。在搜索树中创建一个新的节点Nn作为N的一个新子节点。Nn的局面就是节点N在执行了动作A之后的局面。

<font color=#0099ff>[3]模拟(Simulation)<font color=#>
><font color=#>为了让Nn得到一个初始的评分。我们从Nn开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为Nn的初始评分。一般使用胜利/失败来作为评分，只有1或者0。

<font color=#0099ff>[4]反向传播(Backpropagation)<font color=#>
><font color=#>在Nn的模拟结束之后，它的父节点N以及从根节点到N的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分。如果在[1]的选择中直接发现了一个游戏结局的话，根据该结局来更新评分。每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。

* #### 第二种解释：

><font color=#>传统意义上讲，算法名字带有蒙特卡洛的意思就是，他对搜索空间的搜索都是随机给一个方向的，譬如说蒙塔卡罗算圆周率，就是在一个正方形里面随机取点，看看落在圆里面有多少。蒙特卡洛光线追踪，在需要对环境积分的时候随机取角度射光线。蒙特卡洛走迷宫，随便走。

* 一次迭代的图例
![](https://pic3.zhimg.com/50/v2-2e1fec1b6a9b54562bb38c0e342dc096_hd.png)

* #### 上面的说法通俗的可理解为：
> <font color=#>先在初始棋局下找到一个最为迫切（如存在没有被评价过的节点，则为最迫切的。如都被评价过，则根据每个节点的通过UCB公式计算得出的UCB值的大小进行选择，选择UCB值较大的节点进行拓展）需要拓展的次级节点，然后在次级节点上开始<font color=#0099ff>__随机落子(？)（具体需要看文档和代码，应该有套模板等一系列的落子策略）__，<font color=#>最终得到一个胜负。通过这个胜率更新其所有父节点的胜率，然后再次从更新胜率后的初始局面开始下一轮的计算。

* UCB公式

> <font color=#>这个走法更新后的胜率 = 这个走法以往的胜率 + 平方根(2 * log(所有走法模拟的次数) / 这个走法模拟的次数)

* UCT算法

> <font color=#>UCT是基于UCB算法的，一层代表自己方，另一层就是代表对方，就这样交替扩展，对每一个要模拟的走法的选择上，也是一层一层的计算UCB，选择最大的一个，直到进入了叶节点，这时开始模拟，结果更新到所有同队的父节点。最后的过程一样，选择第一级子节点的UCB值最大的一个作为最佳走法。不但考虑当前的走法，也考虑对方的对策，一层一层的交替考虑。

>UCT算法是一种特殊的蒙特卡洛搜索算法，它由树内选择策略、缺省仿真策略和仿真结果回传三部分组成。

>最终结束时，树的形状为有希望的值较大的分支迭代的很深，而希望比较小的分支，搜索的深度很浅。